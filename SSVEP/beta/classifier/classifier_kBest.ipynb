{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading /home/moraski/faculdade/patter-recog/SSVEP/beta/classifier/../beta_epo1.fif ...\n",
      "Isotrak not found\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2996.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "160 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading /home/moraski/faculdade/patter-recog/SSVEP/beta/classifier/../beta_epo2.fif ...\n",
      "Isotrak not found\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2996.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "160 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_6445/2788898231.py:3: RuntimeWarning: This filename (../beta_epo1.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  dataset1 = mne.read_epochs(\"../beta_epo1.fif\")\n",
      "/tmp/ipykernel_6445/2788898231.py:4: RuntimeWarning: This filename (../beta_epo2.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  dataset2 = mne.read_epochs(\"../beta_epo2.fif\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160, 9, 750), (160, 9, 750))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "dataset1 = mne.read_epochs(\"../beta_epo1.fif\")\n",
    "dataset2 = mne.read_epochs(\"../beta_epo2.fif\")\n",
    "# entendimento dos dados\n",
    "dataset1.get_data().shape, dataset2.get_data().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação por isolamento de frequência\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracia dataset 1: 20.0 %\n",
      "Accuracia dataset 2: 16.25 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hits_dataset1 = 0\n",
    "hits_dataset2 = 0\n",
    "\n",
    "y_dataset1 = np.load(\"../../datasets/beta/labels1.npy\") #carrega os labels\n",
    "y_dataset2 = np.load(\"../../datasets/beta/labels2.npy\")\n",
    "\n",
    "threshold = 0.20 #threshold de 20%\n",
    "targets = [float(item) for item in dataset1.event_id.keys()] #pega os targets\n",
    "\n",
    "for i in range(len(dataset1)): \n",
    "    psd_dataset1 = dataset1[i].compute_psd(method='welch', fmin=7, fmax=17, verbose=False) #pega a psd\n",
    "    psd_dataset2 = dataset2[i].compute_psd(method='welch', fmin=7, fmax=17, verbose=False) #pega a psd\n",
    "\n",
    "    for target in targets: #para cada target\n",
    "        fmin = target - threshold #calcula o fmin e fmax\n",
    "        fmax = target + threshold \n",
    "\n",
    "        # Dataset 1\n",
    "        features_dataset1 = psd_dataset1.get_data(fmin=fmin, fmax=fmax) #pega os dados\n",
    "        X_dataset1 = np.array(features_dataset1) #transforma em array\n",
    "        max_frequency_dataset1 = np.max(X_dataset1, axis=1) #pega o maximo de cada linha\n",
    "\n",
    "        if np.any((max_frequency_dataset1 >= fmin) & (max_frequency_dataset1 <= fmax)): #se o maximo estiver entre o fmin e fmax\n",
    "            hits_dataset1 += 1 #acertou\n",
    "\n",
    "        # Dataset 2\n",
    "        features_dataset2 = psd_dataset2.get_data(fmin=fmin, fmax=fmax) #pega os dados\n",
    "        X_dataset2 = np.array(features_dataset2) #transforma em array\n",
    "        max_frequency_dataset2 = np.max(X_dataset2, axis=1) #pega o maximo de cada linha\n",
    "\n",
    "        if np.any((max_frequency_dataset2 >= fmin) & (max_frequency_dataset2 <= fmax)): #se o maximo estiver entre o fmin e fmax\n",
    "            hits_dataset2 += 1 #acertou\n",
    "\n",
    "accuracy_dataset1 = (hits_dataset1 / len(y_dataset1)) * 100  #calcula a acuracia\n",
    "accuracy_dataset2 = (hits_dataset2  / len(y_dataset2)) * 100\n",
    "\n",
    "print(f\"Accuracia dataset 1: {accuracy_dataset1} %\")\n",
    "print(f\"Accuracia dataset 2: {accuracy_dataset2} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação utilizando SelectKBest para seleção de atributos e SVM para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "Acurácia para o Dataset 1: 15.62%\n",
      "Acurácia para o Dataset 2: 6.25%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import SelectKBest, f_classif\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "ch_ideal = [\"PZ\", \"PO3\", \"PO5\", \"PO4\", \"PO6\", \"POZ\", \"O1\", \"OZ\", \"O2\"] # canais ideais\n",
    "\n",
    "X_dataset1 = dataset1.pick_channels(ch_names=ch_ideal, ordered=False)._data # pega os dados dos canais ideais\n",
    "X_dataset2 = dataset2.pick_channels(ch_names=ch_ideal, ordered=False)._data\n",
    "\n",
    "X_dataset1 = dataset1.get_data().reshape(dataset1.get_data().shape[0], dataset1.get_data().shape[1] * dataset1.get_data().shape[2]) # reshape para duas dimensões\n",
    "X_dataset2 = dataset2.get_data().reshape(dataset2.get_data().shape[0], dataset2.get_data().shape[1] * dataset2.get_data().shape[2])\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_dataset1, y_dataset1, test_size=0.2, random_state=42) # divide os dados em treino e teste\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_dataset2, y_dataset2, test_size=0.2, random_state=42)\n",
    "\n",
    "n_features_to_select = 9 # número de features a serem selecionadas\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder # transforma os labels em números\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train1 = label_encoder.fit_transform(y_train1)\n",
    "y_test1 = label_encoder.transform(y_test1)\n",
    "y_train2 = label_encoder.fit_transform(y_train2)\n",
    "y_test2 = label_encoder.transform(y_test2)\n",
    "\n",
    "# SelectKBest com F-value para seleção das features\n",
    "selector1 = SelectKBest(score_func=f_classif, k=n_features_to_select)\n",
    "selector2 = SelectKBest(score_func=f_classif, k=n_features_to_select)\n",
    "\n",
    "X_train_selected1 = selector1.fit_transform(X_train1, y_train1) # seleciona as features\n",
    "X_test_selected1 = selector1.transform(X_test1)\n",
    "X_train_selected2 = selector2.fit_transform(X_train2, y_train2)\n",
    "X_test_selected2 = selector2.transform(X_test2)\n",
    "\n",
    "# SVC\n",
    "svc1 = SVC(kernel=\"linear\", random_state=42, C=1.0, probability=True)\n",
    "svc2 = SVC(kernel=\"linear\", random_state=42, C=1.0, probability=True)\n",
    "\n",
    "svc1.fit(X_train_selected1, y_train1) # treina o classificador\n",
    "svc2.fit(X_train_selected2, y_train2)\n",
    "\n",
    "y_pred1 = svc1.predict(X_test_selected1) # faz a predição\n",
    "y_pred2 = svc2.predict(X_test_selected2)\n",
    "\n",
    "accuracia1 = accuracy_score(y_test1, y_pred1) # calcula a acurácia\n",
    "accuracia2 = accuracy_score(y_test2, y_pred2)\n",
    "\n",
    "print(f\"Acurácia para o Dataset 1: {accuracia1 * 100:.2f}%\")\n",
    "print(f\"Acurácia para o Dataset 2: {accuracia2 * 100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
