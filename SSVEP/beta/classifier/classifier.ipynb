{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading c:\\Users\\caio-\\Documents\\facul\\patter-recog\\SSVEP\\beta\\classifier\\..\\beta_epo1.fif ...\n",
      "Isotrak not found\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2996.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "160 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n",
      "Reading c:\\Users\\caio-\\Documents\\facul\\patter-recog\\SSVEP\\beta\\classifier\\..\\beta_epo2.fif ...\n",
      "Isotrak not found\n",
      "    Found the data of interest:\n",
      "        t =       0.00 ...    2996.00 ms\n",
      "        0 CTF compensation matrices available\n",
      "Not setting metadata\n",
      "160 matching events found\n",
      "No baseline correction applied\n",
      "0 projection items activated\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caio-\\AppData\\Local\\Temp\\ipykernel_22872\\2788898231.py:3: RuntimeWarning: This filename (../beta_epo1.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  dataset1 = mne.read_epochs(\"../beta_epo1.fif\")\n",
      "C:\\Users\\caio-\\AppData\\Local\\Temp\\ipykernel_22872\\2788898231.py:4: RuntimeWarning: This filename (../beta_epo2.fif) does not conform to MNE naming conventions. All epochs files should end with -epo.fif, -epo.fif.gz, _epo.fif or _epo.fif.gz\n",
      "  dataset2 = mne.read_epochs(\"../beta_epo2.fif\")\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((160, 9, 750), (160, 9, 750))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import mne\n",
    "\n",
    "dataset1 = mne.read_epochs(\"../beta_epo1.fif\")\n",
    "dataset2 = mne.read_epochs(\"../beta_epo2.fif\")\n",
    "# entendimento dos dados\n",
    "dataset1.get_data().shape, dataset2.get_data().shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação por isolamento de frequência\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracia dataset 1: 20.0 %\n",
      "Accuracia dataset 2: 16.25 %\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "hits_dataset1 = 0\n",
    "hits_dataset2 = 0\n",
    "\n",
    "y_dataset1 = np.load(\"../../datasets/beta/labels1.npy\") #carrega os labels\n",
    "y_dataset2 = np.load(\"../../datasets/beta/labels2.npy\")\n",
    "\n",
    "threshold = 0.20 #threshold de 20%\n",
    "targets = [float(item) for item in dataset1.event_id.keys()] #pega os targets\n",
    "\n",
    "for i in range(len(dataset1)): \n",
    "    psd_dataset1 = dataset1[i].compute_psd(method='welch', fmin=7, fmax=17, verbose=False) #pega a psd\n",
    "    psd_dataset2 = dataset2[i].compute_psd(method='welch', fmin=7, fmax=17, verbose=False) #pega a psd\n",
    "\n",
    "    for target in targets: #para cada target\n",
    "        fmin = target - threshold #calcula o fmin e fmax\n",
    "        fmax = target + threshold \n",
    "\n",
    "        # Dataset 1\n",
    "        features_dataset1 = psd_dataset1.get_data(fmin=fmin, fmax=fmax) #pega os dados\n",
    "        X_dataset1 = np.array(features_dataset1) #transforma em array\n",
    "        max_frequency_dataset1 = np.max(X_dataset1, axis=1) #pega o maximo de cada linha\n",
    "\n",
    "        if np.any((max_frequency_dataset1 >= fmin) & (max_frequency_dataset1 <= fmax)): #se o maximo estiver entre o fmin e fmax\n",
    "            hits_dataset1 += 1 #acertou\n",
    "\n",
    "        # Dataset 2\n",
    "        features_dataset2 = psd_dataset2.get_data(fmin=fmin, fmax=fmax) #pega os dados\n",
    "        X_dataset2 = np.array(features_dataset2) #transforma em array\n",
    "        max_frequency_dataset2 = np.max(X_dataset2, axis=1) #pega o maximo de cada linha\n",
    "\n",
    "        if np.any((max_frequency_dataset2 >= fmin) & (max_frequency_dataset2 <= fmax)): #se o maximo estiver entre o fmin e fmax\n",
    "            hits_dataset2 += 1 #acertou\n",
    "\n",
    "accuracy_dataset1 = (hits_dataset1 / len(y_dataset1)) * 100  #calcula a acuracia\n",
    "accuracy_dataset2 = (hits_dataset2  / len(y_dataset2)) * 100\n",
    "\n",
    "print(f\"Accuracia dataset 1: {accuracy_dataset1} %\")\n",
    "print(f\"Accuracia dataset 2: {accuracy_dataset2} %\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classificação utilizando RFE para seleção de atributos e SVM para classificação"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n",
      "NOTE: pick_channels() is a legacy function. New code should use inst.pick(...).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\caio-\\AppData\\Local\\Temp\\ipykernel_22872\\3714709817.py:12: FutureWarning: The default for pick_channels will change from ordered=False to ordered=True in 1.5 and this will result in an error because the following channel names are missing:\n",
      "['Pz', 'POz', 'Oz']\n",
      "Either fix your included names or explicitly pass ordered=False.\n",
      "  X_dataset1 = dataset1.pick_channels(ch_names=ch_ideal)._data #pega os dados dos canais ideais\n",
      "C:\\Users\\caio-\\AppData\\Local\\Temp\\ipykernel_22872\\3714709817.py:12: FutureWarning: The default for pick_channels will change from ordered=False to ordered=True in 1.5 and this will result in a change of behavior because the resulting channel order will not match. Either use a channel order that matches your instance or pass ordered=False.\n",
      "  X_dataset1 = dataset1.pick_channels(ch_names=ch_ideal)._data #pega os dados dos canais ideais\n",
      "C:\\Users\\caio-\\AppData\\Local\\Temp\\ipykernel_22872\\3714709817.py:13: FutureWarning: The default for pick_channels will change from ordered=False to ordered=True in 1.5 and this will result in an error because the following channel names are missing:\n",
      "['Pz', 'POz', 'Oz']\n",
      "Either fix your included names or explicitly pass ordered=False.\n",
      "  X_dataset2 = dataset2.pick_channels(ch_names=ch_ideal)._data\n",
      "C:\\Users\\caio-\\AppData\\Local\\Temp\\ipykernel_22872\\3714709817.py:13: FutureWarning: The default for pick_channels will change from ordered=False to ordered=True in 1.5 and this will result in a change of behavior because the resulting channel order will not match. Either use a channel order that matches your instance or pass ordered=False.\n",
      "  X_dataset2 = dataset2.pick_channels(ch_names=ch_ideal)._data\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Acurária para o Dataset 1: 0.00%\n",
      "Acurária para o Dataset 2: 6.25%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import mne\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFE\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "ch_ideal = [\"Pz\", \"PO3\", \"PO5\", \"PO4\", \"PO6\", \"POz\", \"O1\", \"Oz\", \"O2\"] #canais ideais\n",
    "\n",
    "\n",
    "X_dataset1 = dataset1.pick_channels(ch_names=ch_ideal)._data #pega os dados dos canais ideais\n",
    "X_dataset2 = dataset2.pick_channels(ch_names=ch_ideal)._data \n",
    "\n",
    "X_dataset1 = dataset1.get_data().reshape(dataset1.get_data().shape[0], dataset1.get_data().shape[1] * dataset1.get_data().shape[2]) #reshape para duas dimensoes\n",
    "X_dataset2 = dataset2.get_data().reshape(dataset2.get_data().shape[0], dataset2.get_data().shape[1] * dataset2.get_data().shape[2])\n",
    "\n",
    "\n",
    "X_train1, X_test1, y_train1, y_test1 = train_test_split(X_dataset1, y_dataset1, test_size=0.2, random_state=42) #divide os dados em treino e teste\n",
    "X_train2, X_test2, y_train2, y_test2 = train_test_split(X_dataset2, y_dataset2, test_size=0.2, random_state=42)\n",
    "\n",
    "\n",
    "n_features_to_select = 9  #numero de features a serem selecionadas\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder #transforma os labels em numeros\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "y_train1 = label_encoder.fit_transform(y_train1) \n",
    "y_test1 = label_encoder.transform(y_test1)  \n",
    "y_train2 = label_encoder.fit_transform(y_train2)\n",
    "y_test2 = label_encoder.transform(y_test2)\n",
    "\n",
    "\n",
    "svc1 = SVC(kernel=\"linear\") #cria o classificador\n",
    "svc2 = SVC(kernel=\"linear\")\n",
    "\n",
    "rfe1 = RFE(estimator=svc1, step=0.1,n_features_to_select=n_features_to_select) #cria o RFE\n",
    "rfe2 = RFE(estimator=svc2, step=0.1, n_features_to_select=n_features_to_select)\n",
    "\n",
    "X_train_selected1 = rfe1.fit_transform(X_train1, y_train1) #seleciona as features\n",
    "X_test_selected1 = rfe1.transform(X_test1) \n",
    "X_train_selected2 = rfe1.fit_transform(X_train2, y_train2) \n",
    "X_test_selected2 = rfe1.transform(X_test2)\n",
    "\n",
    "\n",
    "svc1.fit(X_train_selected1, y_train1) #treina o classificador\n",
    "svc2.fit(X_train_selected2, y_train2)\n",
    "\n",
    "y_pred1 = svc1.predict(X_test_selected1) #faz a predicao\n",
    "y_pred2 = svc2.predict(X_test_selected2)\n",
    "\n",
    "y_test1 = np.ravel(y_test1) #transforma em array\n",
    "y_pred1 = np.ravel(y_pred1)\n",
    "\n",
    "y_test1 = y_test1.reshape(-1, 1) #reshape para duas dimensoes\n",
    "y_pred1 = y_pred1.reshape(-1, 1)\n",
    "\n",
    "y_test2 = np.ravel(y_test2) \n",
    "y_pred2 = np.ravel(y_pred2)\n",
    "\n",
    "y_test2 = y_test2.reshape(-1, 1)\n",
    "y_pred2 = y_pred2.reshape(-1, 1)\n",
    "\n",
    "\n",
    "accuracia1 = accuracy_score(y_test1, y_pred1) #calcula a acuracia\n",
    "accuracia2 = accuracy_score(y_test2, y_pred2)\n",
    "\n",
    "print(f\"Acurária para o Dataset 1: {accuracia1 * 100:.2f}%\")\n",
    "print(f\"Acurária para o Dataset 2: {accuracia2 * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
